{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification_drugTest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEOKG8z4aRDKvk5jvCebQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benNthen/COMP723_DataMining/blob/main/TextClassification_drugTest_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtWSEBVaHHoO",
        "outputId": "a1c2f762-f06b-4db8-e0c8-159fb6e19c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-21 03:25:08--  https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1133354 (1.1M) [application/x-httpd-php]\n",
            "Saving to: ‘drugLib_raw.zip’\n",
            "\n",
            "drugLib_raw.zip     100%[===================>]   1.08M  2.81MB/s    in 0.4s    \n",
            "\n",
            "2020-10-21 03:25:09 (2.81 MB/s) - ‘drugLib_raw.zip’ saved [1133354/1133354]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwVqNNwRHMkd",
        "outputId": "b95e1e2f-25c0-45a2-cc82-02328fd44dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!unzip drugLib_raw.zip          #unzip data "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drugLib_raw.zip\n",
            "  inflating: drugLibTest_raw.tsv     \n",
            "  inflating: drugLibTrain_raw.tsv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfK2mPGTHPRc"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Import train data as x1 and test data as x2\n",
        "x1 = pd.read_csv('drugLibTrain_raw.tsv', delimiter='\\t') # train set\n",
        "x2 = pd.read_csv('drugLibTest_raw.tsv',delimiter='\\t') # test set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToA2qI0mAm38",
        "outputId": "66b5b57a-13da-42e3-dead-a01564758696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('corpus')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmitizer = WordNetLemmatizer()\n",
        "\n",
        "#lematize will simplify all the words in the corpus's feature column\n",
        "def lemmword(corpus):\n",
        "    L_corpus = []\n",
        "    for i in corpus:\n",
        "        if isinstance(i, str):\n",
        "            tokens = word_tokenize(i)\n",
        "            s = [lemmitizer.lemmatize(word) for word in tokens] # lematizes all tokens\n",
        "            s = [word.lower() for word in s] # convert all letters into lowercases\n",
        "            s = ' '.join(s) # reassembles tokens into one string\n",
        "        else:\n",
        "            s = \"\"\n",
        "        L_corpus.append(s) # gets appended to final list\n",
        "    return L_corpus\n",
        "\n",
        "# function vector that converts strings from datasets into word vector, while filtering out stopwords\n",
        "conv = CountVectorizer(stop_words=stopwords.words('english'))\n",
        "\n",
        "# vectorizer used to convert review strings and returns a Term-Frequency Document Inverse Frequency(TF-DIF)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words = stopwords.words('english'),\n",
        "                             max_features=20000, max_df=0.60,\n",
        "                             min_df=15, smooth_idf=True)\n",
        "\n",
        "# function vector that converts strings from datasets into word vector, while filtering out stopwords\n",
        "conv = CountVectorizer(stop_words=stopwords.words('english'))\n",
        "\n",
        "# vectorizer used to convert review strings and returns a Term-Frequency Document Inverse Frequency(TF-DIF)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words = stopwords.words('english'),\n",
        "                             max_features=20000, max_df=0.60,\n",
        "                             min_df=15, smooth_idf=True)\n",
        "\n",
        "# columns features by their order in the array, used with iloc\n",
        "#'Unnamed'(0), 'urlDrugName'(1), 'rating'(2), 'effectiveness'(3), 'sideEffects'(4),\n",
        "# 'condition'(5), 'benefitsReview'(6), 'sideEffectsReview'(7), 'commentsReview(8)'\n",
        "\n",
        "# extract column named 'rating' from train and test\n",
        "rate_train = x1['rating'].values\n",
        "rate_test = x2['rating'].values\n",
        "\n",
        "# extract all contents of effectiveness by rows, convert to vector-array-list\n",
        "effectiveness1 = conv.fit_transform(x1['effectiveness'].values.tolist()).toarray()\n",
        "effectiveness2 = conv.transform(x2['effectiveness'].values.tolist()).toarray()\n",
        "\n",
        "# extract all values column #4 - sideEffects on the train and test file\n",
        "labels1 = x1.iloc[:, 4].values\n",
        "labels2 = x2.iloc[:, 4].values\n",
        "\n",
        "effRate = []\n",
        "# effRate gives a '5-step rating' characteristic to the data by creating a key-value pair with 'effectiveness' attribute\n",
        "# {'1': 'Ineffective', '2': 'Marginally Effective',\n",
        "#  '3': 'Moderately Effective, '4': 'Considerably Effective',\n",
        "#  '5': 'Highly Effective'}\n",
        "\n",
        "# for train data\n",
        "for effect in x1['effectiveness'].values:\n",
        "    if effect == \"Highly Effective\":\n",
        "        effRate.append([5])\n",
        "    elif effect == \"Moderately Effective\":\n",
        "        effRate.append([3])\n",
        "    elif effect == \"Considerably Effective\":\n",
        "        effRate.append([4])\n",
        "    elif effect == \"Marginally Effective\":\n",
        "        effRate.append([2])\n",
        "    elif effect == \"Ineffective\":\n",
        "        effRate.append([1])\n",
        "\n",
        "ratingScore1 = x1[['rating']].values # use this variable to obtain organized test data set\n",
        "\n",
        "# for test data\n",
        "for effect in x2['effectiveness'].values:\n",
        "    if effect == \"Highly Effective\":\n",
        "        effRate.append([5])\n",
        "    elif effect == \"Moderately Effective\":\n",
        "        effRate.append([3])\n",
        "    elif effect == \"Considerably Effective\":\n",
        "        effRate.append([4])\n",
        "    elif effect == \"Marginally Effective\":\n",
        "        effRate.append([2])\n",
        "    elif effect == \"Ineffective\":\n",
        "        effRate.append([1])\n",
        "\n",
        "ratingScore2 = x2[['rating']].values # use this variable to obtain organized train data set\n",
        "\n",
        "# convert numpy array into a list\n",
        "effectivenessRating1 = x1['effectiveness'].values.tolist()\n",
        "effectivenessRating2 = x2['effectiveness'].values.tolist()\n",
        "\n",
        "# extract and convert effectivenessRating to a vector\n",
        "effRatingVec1 = conv.fit_transform(effectivenessRating1).toarray()\n",
        "effRatingVec2 = conv.transform(effectivenessRating2).toarray()\n",
        "\n",
        "# conversion from pandas to numpy to python list of the 'sideEffectsReview' feature column\n",
        "sideEffComm1 = x1.iloc[:, 7].values.astype('str').tolist()\n",
        "sideEffComm2 = x2.iloc[:, 7].values.astype('str').tolist()\n",
        "\n",
        "# each row of the sideEffectsReviews are pre-processed through lemmword()\n",
        "sideEffLem1 = lemmword(sideEffComm1)\n",
        "sideEffLem2 = lemmword(sideEffComm2)\n",
        "\n",
        "# To get a TF-IDF instead of a count vectoriser\n",
        "sideEffLem1 = vectorizer.fit_transform(sideEffLem1).toarray()\n",
        "sideEffLem2 = vectorizer.transform(sideEffLem2).toarray()\n",
        "\n",
        "# numpy to python list of type unicode ('U')\n",
        "commentsReview1 = x1.iloc[:, 8].values.astype('U').tolist()\n",
        "commentsReview2 = x2.iloc[:, 8].values.astype('U').tolist()\n",
        "\n",
        "# extract and convert effectivenessRating to a vector\n",
        "commentsReviewVec1 = vectorizer.fit_transform(commentsReview1).toarray()\n",
        "commentsReviewVec2 = vectorizer.transform(commentsReview2).toarray()\n",
        "\n",
        "# conflate all reviews into one review set vector\n",
        "train_row = np.concatenate((ratingScore1, effRatingVec1, commentsReviewVec1), axis=1)\n",
        "test_row = np.concatenate((ratingScore2, effRatingVec2, commentsReviewVec2), axis=1)\n",
        "\n",
        "# create label variables for column features with X for train, y for test\n",
        "X = np.asarray(test_row)\n",
        "y = labels2\n",
        "\n",
        "# split train and test data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp4uQc_pE9Rf",
        "outputId": "03ee6d47-e0d9-4cc7-dd36-a80501ffcf52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classBay = GaussianNB()\n",
        "NBmodel = classBay.fit(X_train, y_train)\n",
        "NBpredict = NBmodel.predict(X_test)\n",
        "\n",
        "print(\"Naives Bayes Accuracy:\", metrics.accuracy_score(y_test, NBpredict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naives Bayes Accuracy: 0.2765273311897106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJDyH-P8FC5B",
        "outputId": "933a0a9d-c641-4864-e30c-7ce84e0645bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Neural Network Classifier\n",
        "from sklearn import neural_network\n",
        "\n",
        "ANclf = neural_network.MLPClassifier(max_iter=600, hidden_layer_sizes=(150, 150), early_stopping=False)\n",
        "ANmodel = ANclf.fit(X_train, y_train)\n",
        "ANpredict = ANmodel.predict(X_test)\n",
        "\n",
        "# Neural Network Accuracy\n",
        "print(\"Neural Network Accuracy:\", metrics.accuracy_score(y_test, ANpredict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network Accuracy: 0.37942122186495175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-1PyEwbFGJn",
        "outputId": "c71c634e-8841-4f75-d18e-f5aeb3ff1cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Decision Tree Classifier\n",
        "from sklearn import tree\n",
        "Tmodel = tree.DecisionTreeClassifier()\n",
        "TClas= Tmodel.fit(X_train, y_train)\n",
        "y_dec_tree = Tmodel.predict(X_test)\n",
        "\n",
        "# Decision Tree Classifier Accuracy\n",
        "print(\"Decision Tree Classifier Accuracy:\", metrics.accuracy_score(y_test, y_dec_tree))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decision Tree Classifier Accuracy: 0.3633440514469453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_8zGAtXFI50",
        "outputId": "0f5fe46c-fb29-4468-9e55-49d68aaf52c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "#Random Forest Classifier\n",
        "from sklearn import ensemble\n",
        "\n",
        "RFclf = ensemble.RandomForestClassifier(n_estimators=600, max_leaf_nodes=400, max_depth=100)\n",
        "RFClas = RFclf.fit(X=X_train, y=y_train)\n",
        "y_pred_tree = RFclf.predict(X_test)\n",
        "\n",
        "# Random Forest Classifier Accuracy\n",
        "print(\"RandomForestClassifier Accuracy:\", metrics.accuracy_score(y_test, y_pred_tree))\n",
        "print(classification_report(y_test, y_pred_tree))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier Accuracy: 0.42765273311897106\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "Extremely Severe Side Effects       0.39      0.65      0.49        17\n",
            "            Mild Side Effects       0.36      0.52      0.43        90\n",
            "        Moderate Side Effects       0.42      0.43      0.42        77\n",
            "              No Side Effects       0.57      0.43      0.49        84\n",
            "          Severe Side Effects       0.55      0.14      0.22        43\n",
            "\n",
            "                     accuracy                           0.43       311\n",
            "                    macro avg       0.46      0.43      0.41       311\n",
            "                 weighted avg       0.46      0.43      0.42       311\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3EEAwHhFMPh",
        "outputId": "6f0e2083-e9f4-4191-beaa-6c203a7b77a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "KNclf = KNeighborsClassifier(n_neighbors=20)\n",
        "KNmod = KNclf.fit(X_train, y_train)\n",
        "y_pred_knn = KNmod.predict(X_test)\n",
        "\n",
        "# K-Nearest Neighbor Classifier Accuracy\n",
        "print(\"K-Nearest Neighbor Accuracy:\", metrics.accuracy_score(y_test, y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K-Nearest Neighbor Accuracy: 0.5466237942122186\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "Extremely Severe Side Effects       0.48      0.82      0.61        17\n",
            "            Mild Side Effects       0.51      0.64      0.57        90\n",
            "        Moderate Side Effects       0.55      0.61      0.58        77\n",
            "              No Side Effects       0.67      0.49      0.57        84\n",
            "          Severe Side Effects       0.45      0.23      0.31        43\n",
            "\n",
            "                     accuracy                           0.55       311\n",
            "                    macro avg       0.53      0.56      0.53       311\n",
            "                 weighted avg       0.55      0.55      0.54       311\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}